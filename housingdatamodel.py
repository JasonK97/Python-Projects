# -*- coding: utf-8 -*-
"""housingDataModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PG3wOQS1k4Kwu1r3C5ymw-wEix8mDKxK
"""

import pandas as pd
import altair as alt
housing = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/housing.csv')

housing

alt.data_transformers.disable_max_rows()
alt.Chart(housing).mark_point().encode(
    x='price',
    y='sqft_living',
    color='waterfront'
)

alt.Chart(housing).mark_point().encode(
    x='price',
    y='sqft_living15',
    color='waterfront'
)

alt.Chart(housing).mark_point().encode(
    x='price',
    y='bathrooms'
)

alt.Chart(housing).mark_point().encode(
    x='price',
    y='view',
    color='waterfront'
)

alt.Chart(housing).mark_point().encode(
    y='price',
    column='zipcode'
)

alt.Chart(housing).mark_point().encode(
    y='price',
    column='bathrooms'   
)

alt.Chart(housing).mark_point().encode(
    y='price',
    column='grade'   
)

alt.Chart(housing).mark_point().encode(
    y='price',
    x='sqft_above',
    color='waterfront' 
)

# Important things: Zipcode, bathrooms, sqft_living/waterfront, grade, sqft_above/waterfront

# Handle dates so it'll work correctly
housing['date'] = housing['date'].str[:4]
housing['date'] = housing.date.astype(int)
housing.head()

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

# Normalize the important features in the data thta need it
housing['zipcode'] = scaler.fit_transform(housing[['zipcode']])
housing['bedrooms'] = scaler.fit_transform(housing[['bedrooms']])
housing['bathrooms'] = scaler.fit_transform(housing[['bathrooms']])
housing['grade'] = scaler.fit_transform(housing[['grade']])
housing['view'] = scaler.fit_transform(housing[['view']])
housing['floors'] = scaler.fit_transform(housing[['floors']])
housing['condition'] = scaler.fit_transform(housing[['condition']])
housing['waterfront'] = scaler.fit_transform(housing[['waterfront']])
housing['sqft_living'] = scaler.fit_transform(housing[['sqft_living']])
housing['sqft_lot'] = scaler.fit_transform(housing[['sqft_lot']])
housing['sqft_above'] = scaler.fit_transform(housing[['sqft_above']])
housing['sqft_basement'] = scaler.fit_transform(housing[['sqft_basement']])
housing['sqft_living15'] = scaler.fit_transform(housing[['sqft_living15']])
housing['sqft_lot15'] = scaler.fit_transform(housing[['sqft_lot15']])

housing.head()

from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn import metrics
import xgboost as xgb


features = ['sqft_living', 'waterfront', 'condition', 'grade', 'zipcode', 'sqft_living15', 'sqft_above']

X = pd.get_dummies(housing[features], drop_first=True)
y = housing['price']

# Split our data into training and test data, with 30% reserved for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)

# regressor = xgb.XGBRegressor(objective = 'reg:linear', max_depth = 20, alpha = 10, n_estimators = 20)
# regressor.fit(X_train, y_train)

# prediction = regressor.predict(X_test)
clf = XGBRegressor(max_depth=5)

clf.fit(X_train, y_train)

# print(regressor.score(X_test, y_test))

prediction = clf.predict(X_test)
prediction

from sklearn.metrics import mean_squared_error

rmse = mean_squared_error(y_test, prediction, squared=False)
rmse

from sklearn.metrics import r2_score

r2_final = r2_score(y_test, prediction)
r2_final

